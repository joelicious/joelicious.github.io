# Homelab Infrastructure

While not required to have a homelab for software development, having one can be a valueable 
learning experience. The intent of this guide is to document the steps to provision an 
OpenShift environment that can host a variety of Open Source integration technologies. In addition,
it will document steps to make your OpenShift cluster accessible from outside your home
network.

## Homelab Architecture

The following is a high level diagram of the hardware for my homelab.

image:2022-okc-homelab.png[Architecture] 

In the top left of the diagram is the https://store.ui.com/collections/unifi-network-unifi-os-consoles/products/udm-pro[Unifi Dream Machine Pro (UDM-Pro)]. This router has the ability
to create multiple networks and handles the routing between each network. For the purposes of this
overview, there are two networks created:

* `192.168.1.x` : handles all non-homelab traffic in my home. This would include Network APs and general 
home stuff.
* `10.10.10.x` : homelab network traffic, which at this point is an OpenShift Cluster.

The https://store.ui.com/collections/operator-edgemax-switches/products/edgeswitch-24-250w[EdgeSwitch PoE+ 24] 
is the switch that I use for `192.168.1.x` network. While there are a number 
devices attached to this network, for the purposes of this guide there is a FreeNas and Work Laptop.
There is a FreeNAS baremetal node that I hope to encorporate into the OpenShift Cluster in the 
future. My Work Laptop exists on the `192.168.1.x` network and can route to the `10.10.10.x` network as 
needed.

For the homelab network, a https://mikrotik.com/product/crs317_1g_16s_rm[Mikrotik Router Switch] 
was used due to its ability to switch traffic at 10 gbe speeds. Connected to this Mikrotik Router 
Switch are three baremetal computers which will become hosts to the OpenShift Cluster.

### Architectural Decisions

1. Traffic for the homelab should not interfere with other home network traffic.
2. Traffic within the homelab network should switch at 10 gbe speeds.
3. Use of a virtualization platform on the baremetal computers.
4. Hardware should meet or exceed minimum hardware requirements for OpenShift.

## Virtualization Platforms

There are a number of choices that were considered for a homelab virtualization platform.

A. https://www.redhat.com/en/technologies/virtualization/enterprise-virtualizationcl[Red Hat Virtualization]: Based on KVM.
B. https://www.proxmox.com/en/proxmox-ve[ProxMox VE]: Also based on KVM. 
C. https://www.vmware.com/products/esxi-and-esx.html[VMWare ESXi]: Costs money / not open source.

VMWare ESXi was evaluated and quickly reached the limit of managing virtual machines across 
multiple hosts was a feature that not included in the free version. ProxMox was the next
to be evaluated and found the overall installation and provisioning of virtual machines to 
be fairly straightforward. I also was able to cluster the three nodes on my network, making 
it easier to manage all VMs in a central location.

Red Hat Virtualization is also a viable option and I hope to evaluate in a future homelab execise.
My experience with ProxMox was so positive that I just decided to go with that platform.

### Minimum Requirements

As mentioned in the documentation for the current OpenShift release (4.10.3), the following
is the minimum hardware requirements:

* Control plane nodes: At least 4 CPU cores, 16.00 GiB RAM, 120 GB disk size for every supervisor.
* Workers: At least 2 CPU cores, 8.00 GiB RAM, 120 GB disk size for each worker
* Single Node OpenShift (SNO): One host is required with at least 4 CPU cores, 16.00 GiB of RAM, and 
120 GB of disk size storage.
* Also note that each host's disk write speed should meet the minimum requirements to run OpenShift.

### ProxMox Configuration

There are a number of guides that can walk you through the provisioning of ProxMox VE. For a deep 
dive on how to install, the following https://forum.proxmox.com/threads/proxmox-beginner-tutorial-how-to-set-up-your-first-virtual-machine-on-a-secondary-hard-disk.59559/[guide is very helpful].

Here is a simplified set of steps that I used:

* Download the latest ISO from: https://www.proxmox.com/en/downloads[https://www.proxmox.com/en/downloads]
* Follow the install steps to provision on bare metal.
* Configure with a static IP (`10.10.10.10`, `10.10.10.11`, `10.10.10.12`) and I choose the following 
hostnames (`pve1`, `pve2`, `pve3`).

Upon completing the install across all three nodes, then follow the steps in the GUI to
"Create Cluster". I found the https://pve.proxmox.com/wiki/Cluster_Manager[Cluster Manager] to be quite 
useful as all hosts were presented in a single location. Additional features such as easy
migration of virtual machines and the ability to make cluster-wide configurations are 
just some of the features that make it worth it to cluster.

## OpenShift Assisted Installer

This https://cloud.redhat.com/blog/how-to-use-the-openshift-assisted-installer[guide] was contructed 
for an OpenShift Assisted Cluster install. The basic premise of this approach is that you create / 
declare your cluster in the https://console.redhat.com/openshift[Red Hat Hybrid Cloud Console].

Upon selecting "Create cluster", then select "Datacenter" and then "Bare Metal (x86_64)". At this
point there is an option to select "Assisted Installer (Technology Preview)". In the subsequent
dialog, there is an option to name the Cluster. Just remember that you may find the need to find
your cluster in a Red Hat Hybrid Cloud Console search, so it would be advantageous to name your
cluster something unique (stay away from ocp / playground!).

In my case, I named my cluster after my hometown Oklahoma City (okc). For a base domain, I have 
a registered domain name and also have intentions to access this cluster from outside my homelab, 
so I used my registered domain. Also in this dialog, there is an option to install single node
OpenShift (SNO) or on arm64 CPU architectures. Since my homelab has multiple hosts and is x86
based, I did not select these options.

In the next dialog, the configuration of hosts is covered. Upon selecting "Add Hosts", a pop
up dialog is presented where you can download an Discovery ISO image. I leveraged the Full
Image file with the hopes that this would speed up the OpenShift provisioning process.

Upon downloading the image, it is important to make this image available to each node
in the ProxMox VE cluster. For each node, select the local disk and then the "ISO Images"
button in the configuration table. This is depicted in the following figure:

image:2022-Proxmox-ISOImages.png[ProxMox Upload Images]

By uploading the ISO, it will then be available to the create VM process dialogs.

## ProxMox Provision

Once you have the ISO Images added to the filesystems, then it is just a matter of creating VMs 
for the OpenShift installer. The beauty of the assisted installer is that it will auto-choose 
the role of the each VM/Node based on its hardware profile.

Since my homelab has 3 bare metal nodes, my approach has been to allocate a control-plane-sized
VM on each bare metal node giving some notion of high availability. I then allocate the balance
of the hardware resources to an additional VM.

Specifically, my three hosts have the following hardware profiles:

1. `pve1` : 40 VCPUs, 128 GiB Ram, 1 TB
2. `pve2` : 16 VCPUs, 64 GiB Ram, 500 GiB disk
3. `pve3` : 8 VCPUs, 64 GiB Ram, 500 GiB disk

Which is then decomposed into the following VMs:

1. `pve1 \ ocp1` : 4 VCPU, 16 GiB Ram, 120 GiB disk
2. `pve1 \ ocp4` : 36 VCPU, 112 GiB Ram, 700 GiB disk
3. `pve2 \ ocp2` : 4 VCPU, 16 GiB Ram, 120 GiB disk
4. `pve2 \ ocp5` : 12 VCPU, 48 GiB Ram, 240 GiB disk
5. `pve3 \ ocp3` : 4 VCPU, 16 GiB Ram, 120 GiB disk
6. `pve3 \ ocp6` : 4 VCPU, 48 GiB Ram, 240 GiB disk

Upon provisioning the VMs with the provided ISO Image, the hosts will register themselves
with the Red Hat Hybrid Cloud Console, roles are assigned, and then the OpenShift cluster
is installed across the hosts.

The following diagram depicts how the ProxMox Console looks after the VMs are deployed 
and started.

image:2022-ProxMox-Console-Deployed.png[ProxMox Console after Deployment]

## Red Hat Hybrid Cloud Console

As mentioned, the VMs will continue to provision and then finish the install of the OpenShift
Platform. Upon completion, the console view provides access information to your own cluster, 
along with guidance on updating your DNS Server or updating a local `/etc/hosts` 
or `/etc/resolv.conf`.

As an example, the Cluster Console will look similar to the following:

image:2022-RH-Cloud-Console.png[Red Hat Hybrid Cloud Console Complete]

The Cluster utilization should match up to the amount of resources allocated to the
VMs in the ProxMox Cluster, as picted in the following:

image:2022-OpenShift-utilization.png[Openshift Utilization]

## Making Your OpenShift Cluster Available outside Homelab

TODO[TODO]

